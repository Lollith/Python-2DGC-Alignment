{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#peak local max=> coordonnates\n",
    "# k-distance\n",
    "# Pour chaque point, on trie les distances vers tous les autres points \n",
    "# et on garde la distance vers le k-ième plus proche voisin (pas nécessairement le centroïde).\n",
    "# Sert à capturer la densité locale autour d’un point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "path_to_scr_folder=os.path.join(os.path.dirname(os.path.abspath('')), 'src')\n",
    "sys.path.append(path_to_scr_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import read_chroma\n",
    "from peak_detection import peak_detection\n",
    "import numpy as np\n",
    "import projection\n",
    "import plot\n",
    "import baseline_correction\n",
    "import netCDF4 as nc\n",
    "import matplotlib.pyplot as plt\n",
    "import peak_detection\n",
    "from kneed import KneeLocator\n",
    "from read_chroma import read_chromato_and_chromato_cube\n",
    "# from peak_detection import peak_detection\n",
    "import peak_detection\n",
    "from baseline_correction import chromato_reduced_noise\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/camille/Documents/app/data/input/A-F-028-817822-droite-ReCIVA.cdf'\n",
    "# estimation with filtering_factor_estimation.ipynb\n",
    "mod_time = 1.7\n",
    "filtering_factor = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic_chromato, time_rn, spectra_obj = read_chroma.read_chroma(file_path, 1.7, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "import pybaselines\n",
    "\n",
    "def chromato_reduced_noise(chromato): \n",
    "    tmp = np.empty_like(chromato)\n",
    "    for i in range(tmp.shape[1]):\n",
    "        tmp[:, i] = savgol_filter(\n",
    "           chromato[:, i] - pybaselines.whittaker.asls(chromato[:, i],\n",
    "                                                       lam=1000.0, # 1000\n",
    "                                                       p=0.1)[0], #0.05\n",
    "           window_length=5,  # 5, 11 pour un lissage + fort\n",
    "           polyorder=2,\n",
    "           mode='nearest')\n",
    "    tmp[tmp < .0] = 0\n",
    "    return tmp\n",
    "\n",
    "# def chromato_reduced_noise(chromato, savgol_window=5, lam=3000.0, p=0.05):\n",
    "#     \"\"\"Version améliorée avec meilleure réduction du bruit.\"\"\"\n",
    "#     tmp = np.empty_like(chromato)\n",
    "#     for i in range(tmp.shape[1]):\n",
    "#         # Correction de ligne de base\n",
    "#         baseline_corrected = chromato[:, i] - pybaselines.whittaker.asls(\n",
    "#             chromato[:, i], lam=lam, p=p)[0]\n",
    "        \n",
    "#         # Premier lissage Savitzky-Golay\n",
    "#         smoothed = savgol_filter(baseline_corrected, savgol_window, 2, mode='nearest')\n",
    "        \n",
    "#         # Utiliser une moyenne mobile pour réduire davantage le bruit\n",
    "#         # (peut être remplacée par un second Savitzky-Golay avec différents paramètres)\n",
    "#         from scipy.ndimage import uniform_filter1d\n",
    "#         smoothed = uniform_filter1d(smoothed, size=5)\n",
    "        \n",
    "#         # Seuillage pour supprimer le bruit de fond\n",
    "#         noise_threshold = np.std(smoothed) * 0.1  # Ajuster selon votre cas\n",
    "#         smoothed[smoothed < noise_threshold] = 0\n",
    "        \n",
    "#         tmp[:, i] = smoothed\n",
    "    \n",
    "#     # Élimination des valeurs négatives\n",
    "#     tmp[tmp < 0.0] = 0\n",
    "    \n",
    "#     return tmp\n",
    "\n",
    "# def wavelet_denoising(chromato):\n",
    "#     \"\"\"Débruitage par ondelettes.\"\"\"\n",
    "#     from pywt import wavedec, waverec, threshold\n",
    "    \n",
    "#     denoised = np.empty_like(chromato)\n",
    "#     for i in range(chromato.shape[1]):\n",
    "#         # Décomposition en ondelettes\n",
    "#         coeffs = wavedec(chromato[:, i], 'sym8', level=3)\n",
    "        \n",
    "#         # Seuillage des coefficients\n",
    "#         for j in range(1, len(coeffs)):\n",
    "#             coeffs[j] = threshold(coeffs[j], value=np.std(coeffs[j])*0.8, mode='soft')\n",
    "        \n",
    "#         # Reconstruction\n",
    "#         denoised[:, i] = waverec(coeffs, 'sym8')\n",
    "    \n",
    "#     denoised[denoised < 0] = 0\n",
    "#     return denoised\n",
    "\n",
    "\n",
    "chromato_tic_preprocessed = chromato_reduced_noise(tic_chromato)\n",
    "# chromato_tic_preprocessed = wavelet_denoising(tic_chromato)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering_factor = 50 #TODO\n",
    "sigma = skimage.restoration.estimate_sigma(chromato_tic_preprocessed, channel_axis=None)\n",
    "# chromato_tic, time_rn, chromato_cube, sigma, mass_range = read_chromato_and_chromato_cube(file_path, mod_time, pre_process=True)\n",
    "dynamic_threshold_fact = filtering_factor * sigma * 100 / np.max(chromato_tic_preprocessed)\n",
    "dynamic_threshold_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.visualizer((tic_chromato, time_rn), title=\"chromato\", log_chromato=False,mod_time=1.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks_coordinates = peak_detection.peak_detection((chromato_tic_preprocessed, time_rn, None), spectra=None, chromato_cube=None, \n",
    "                                                  dynamic_threshold_fact=0.01,\n",
    "                                                ABS_THRESHOLDS=None, \n",
    "                                                method=\"peak_local_max\", \n",
    "                                                mode=\"tic\", \n",
    "                                                cluster=False, \n",
    "                                                unique=False)\n",
    "len(peaks_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates_in_chromato = projection.matrix_to_chromato(peaks_coordinates, time_rn, mod_time, chromato_tic_preprocessed.shape)\n",
    "plot.visualizer((chromato_tic_preprocessed, time_rn), title=\"chromato\", log_chromato=False, points=coordinates_in_chromato, mod_time=1.70)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determiner eps avec k-distance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prétraitement : normalisation des données car DBSCAN est sensible à l'échelle, et RT1 et RT2 sont sur des échelles différentes\n",
    "# En normalisant, les deux dimensions ont une moyenne 0 et un écart-type 1, donc elles sont sur la même échelle, ce qui évite un biais de distance.\n",
    "data_scaled = StandardScaler().fit_transform(peaks_coordinates[:, :2]) \n",
    "# data_scaled = peaks_coordinates[:, :2]\n",
    "\n",
    "# Paramètre k (= minPts pour DBSCAN)\n",
    "# k = 4  # Valeur adaptée pour GC×GC-MSi\n",
    "k = 4\n",
    "\n",
    "# Calcul des k-distances\n",
    "# instancie au k neighbors\n",
    "neighbors = NearestNeighbors(n_neighbors=k)\n",
    "# entraine les data, creer une structure de recherche du voisin\n",
    "neighbors_fit = neighbors.fit(data_scaled)\n",
    "# aplique la recherche des voisins a chaque point\n",
    "distances, indices = neighbors_fit.kneighbors(data_scaled)\n",
    "# tableau contenant , pour chaque point, la distance de ces k voisins\n",
    "k_distances = np.sort(distances[:, k-1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation pour identifier le coude\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(k_distances)), k_distances)\n",
    "plt.xlabel('Points triés par k-distance')\n",
    "plt.ylabel(f'Distance au {k}-ième plus proche voisin')\n",
    "plt.title('Courbe des k-distances')\n",
    "plt.grid(True)\n",
    "\n",
    "# Ajouter des lignes horizontales pour faciliter la sélection\n",
    "percentiles = [50, 75, 90, 95]\n",
    "for p in percentiles:\n",
    "    d = np.percentile(k_distances, p)\n",
    "    plt.axhline(y=d, color='r', linestyle='--', alpha=0.5)\n",
    "    plt.text(len(k_distances)*0.02, d*1.02, f'{p}th percentile: {d:.4f}')\n",
    "\n",
    "# Zoom\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(k_distances))[200:], k_distances[200:])\n",
    "plt.xlabel('Points triés par k-distance (zoom)')\n",
    "plt.ylabel(f'Distance au {k}-ième plus proche voisin')\n",
    "plt.title('Identification du point d\\'inflexion pour eps')\n",
    "plt.grid(True)\n",
    "\n",
    " # Trouver automatiquement le \"coude\" dans la courbe\n",
    "knee = KneeLocator(\n",
    "        range(len(k_distances)), \n",
    "        k_distances, \n",
    "        S=1.0, \n",
    "        curve=\"convex\", \n",
    "        direction=\"increasing\"\n",
    "    )\n",
    "\n",
    "\n",
    "optimal_eps = k_distances[knee.knee]\n",
    "print(f\"Valeur optimale pour eps : {optimal_eps}\")\n",
    "plt.axhline(y=optimal_eps, color='r', linestyle='--', \n",
    "                label=f'optimal eps = {optimal_eps:.4f}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Interprétation du graphique :\n",
    "\n",
    "Les points avant le coude correspondent généralement aux pics bien définis\n",
    "Les points après le coude peuvent correspondre à du bruit ou des pics très isolés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilisation Dbscan avec optimal eps\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "# Application de DBSCAN avec les paramètres choisis\n",
    "dbscan = DBSCAN(eps=optimal_eps, min_samples=k).fit(data_scaled)\n",
    "# attribu  un entier a 1 cluster et  -1 a un bruit, chaque point se voit attribuer le label du cluster auquel il appartient\n",
    "labels = dbscan.labels_\n",
    "\n",
    "\n",
    "# Statistiques basiques sur le clustering\n",
    "# compte le nombre d'étiquettes uniques et ajuste ce compte en soustrayant 1 si le bruit est présent => obtenir le nombre de clusters valides.\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise = list(labels).count(-1)\n",
    "print(f\"Nombre de clusters: {n_clusters}\")\n",
    "print(f\"Nombre de points considérés comme bruit: {n_noise} ({n_noise/len(labels)*100:.1f}%)\")\n",
    "\n",
    "# Visualisation des clusters sur le plan RT1×RT2\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Pour les points de bruit, recreer l array avec des booleens\n",
    "noise_mask = labels == -1\n",
    "plt.scatter(data_scaled[noise_mask, 0], data_scaled[noise_mask, 1], color='gray', alpha=0.3, s=10, label='Bruit')\n",
    "# Pour les clusters\n",
    "unique_labels = set(labels)\n",
    "unique_labels.discard(-1)\n",
    "for label in unique_labels:\n",
    "    cluster_mask = labels == label\n",
    "    plt.scatter(data_scaled[cluster_mask, 0], data_scaled[cluster_mask, 1], alpha=0.7, s=30, label=f'Cluster {label}')\n",
    "\n",
    "plt.xlabel('RT1')\n",
    "plt.ylabel('RT2')\n",
    "plt.title(f'Résultats DBSCAN avec eps={optimal_eps}, minPts={k}')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimation du eps clusters/ bruit\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "min_pts = 4\n",
    "eps_values = np.linspace(0.1, 0.5, 10)  # 10 valeurs de eps\n",
    "fig, axes = plt.subplots(2, len(eps_values) // 2, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, eps in enumerate(eps_values):\n",
    "    clusters = DBSCAN(eps=eps, min_samples=min_pts).fit_predict(data_scaled)  # fit + labels\n",
    "\n",
    "    # Nombre de clusters et points de bruit\n",
    "    n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
    "    n_noise = list(clusters).count(-1)\n",
    "\n",
    "    ax = axes[i]\n",
    "    noise_mask = clusters == -1\n",
    "\n",
    "    # Bruit\n",
    "    ax.scatter(data_scaled[noise_mask, 0], data_scaled[noise_mask, 1],\n",
    "               color='gray', alpha=0.3, s=10, label='Bruit')\n",
    "\n",
    "    # Clusters\n",
    "    unique_labels = set(clusters)\n",
    "    unique_labels.discard(-1)\n",
    "    for label in unique_labels:\n",
    "        cluster_mask = clusters == label\n",
    "        ax.scatter(data_scaled[cluster_mask, 0], data_scaled[cluster_mask, 1],\n",
    "                   alpha=0.7, s=30, label=f'Cluster {label}')\n",
    "\n",
    "    ax.set_xlabel('RT1')\n",
    "    ax.set_ylabel('RT2')\n",
    "    ax.set_title(f'eps:{eps:.3f}, clusters:{n_clusters}, bruit:{n_noise}')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimation du eps clusters/ bruit\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "min_pts = 4\n",
    "eps_values = np.linspace(0.15, 0.25, 10)  # 10 valeurs de eps\n",
    "fig, axes = plt.subplots(2, len(eps_values) // 2, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, eps in enumerate(eps_values):\n",
    "    clusters = DBSCAN(eps=eps, min_samples=min_pts).fit_predict(data_scaled)  # fit + labels\n",
    "\n",
    "    # Nombre de clusters et points de bruit\n",
    "    n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
    "    n_noise = list(clusters).count(-1)\n",
    "\n",
    "    ax = axes[i]\n",
    "    noise_mask = clusters == -1\n",
    "\n",
    "    # Bruit\n",
    "    ax.scatter(data_scaled[noise_mask, 0], data_scaled[noise_mask, 1],\n",
    "               color='gray', alpha=0.3, s=10, label='Bruit')\n",
    "\n",
    "    # Clusters\n",
    "    unique_labels = set(clusters)\n",
    "    unique_labels.discard(-1)\n",
    "    for label in unique_labels:\n",
    "        cluster_mask = clusters == label\n",
    "        ax.scatter(data_scaled[cluster_mask, 0], data_scaled[cluster_mask, 1],\n",
    "                   alpha=0.7, s=30, label=f'Cluster {label}')\n",
    "\n",
    "    ax.set_xlabel('RT1')\n",
    "    ax.set_ylabel('RT2')\n",
    "    ax.set_title(f'eps:{eps:.3f}, clusters:{n_clusters}, bruit:{n_noise}')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilisation Dbscan avec eps =0.161\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "# Application de DBSCAN avec les paramètres choisis\n",
    "dbscan = DBSCAN(eps=0.161, min_samples=k).fit(data_scaled)\n",
    "# attribu  un entier a 1 cluster et  -1 a un bruit, chaque point se voit attribuer le label du cluster auquel il appartient\n",
    "labels = dbscan.labels_\n",
    "\n",
    "\n",
    "# Statistiques basiques sur le clustering\n",
    "# compte le nombre d'étiquettes uniques et ajuste ce compte en soustrayant 1 si le bruit est présent => obtenir le nombre de clusters valides.\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise = list(labels).count(-1)\n",
    "print(f\"Nombre de clusters: {n_clusters}\")\n",
    "print(f\"Nombre de points considérés comme bruit: {n_noise} ({n_noise/len(labels)*100:.1f}%)\")\n",
    "\n",
    "# Visualisation des clusters sur le plan RT1×RT2\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Pour les points de bruit, recreer l array avec des booleens\n",
    "noise_mask = labels == -1\n",
    "plt.scatter(data_scaled[noise_mask, 0], data_scaled[noise_mask, 1], color='gray', alpha=0.3, s=10, label='Bruit')\n",
    "# Pour les clusters\n",
    "unique_labels = set(labels)\n",
    "unique_labels.discard(-1)\n",
    "for label in unique_labels:\n",
    "    cluster_mask = labels == label\n",
    "    plt.scatter(data_scaled[cluster_mask, 0], data_scaled[cluster_mask, 1], alpha=0.7, s=30, label=f'Cluster {label}')\n",
    "\n",
    "plt.xlabel('RT1')\n",
    "plt.ylabel('RT2')\n",
    "plt.title(f'Résultats DBSCAN avec eps={0.161}, minPts={k}')\n",
    "# plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Évaluation :\n",
    "Vérifier si les pics connus d'un même composé sont regroupés correctement\n",
    "Comparez le nombre de clusters et leur cohérence chimique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "#indice de silhouette simplifiee\n",
    "\n",
    "\n",
    "# a(i) : la distance moyenne du point à tous les autres points de son propre cluster (cohésion) (intracluster)\n",
    "# b(i) : la distance moyenne minimale du point aux points du cluster voisin le plus proche (séparation)(extra cluster)\n",
    "\n",
    "# L'indice de silhouette s(i) = (b(i) - a(i)) / max(a(i), b(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifier la qualite avec l indice de silhouette\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "\n",
    "def simplified_silhouette_for_dbscan(X, labels):\n",
    "    \"\"\"Calcule un indice de silhouette simplifié adapté à DBSCAN\"\"\"\n",
    "    unique_labels = set(labels)\n",
    "    unique_labels.discard(-1)\n",
    "    \n",
    "    #  Si moins de 2 clusters, pas de sens de calculer la silhouette\n",
    "    if len(unique_labels) < 2:\n",
    "        return np.nan\n",
    "    \n",
    "    # Points qui ne sont pas du bruit\n",
    "    valid_points = labels != -1\n",
    "    X_valid = X[valid_points]\n",
    "    labels_valid = labels[valid_points]\n",
    "    \n",
    "    # Utiliser la fonction standard de sklearn si possible\n",
    "    try:\n",
    "        return silhouette_score(X_valid, labels_valid)\n",
    "    except:\n",
    "        # Version simplifiée (si trop de clusters ou autres problèmes)\n",
    "        silhouette_scores = []\n",
    "        cluster_centers = {}\n",
    "        \n",
    "        # Calculer les centroïdes\n",
    "        for label in unique_labels:\n",
    "            cluster_points = X[labels == label]\n",
    "            cluster_centers[label] = np.mean(cluster_points, axis=0)\n",
    "        \n",
    "        # Pour chaque point non-bruit\n",
    "        for i, (point, label) in enumerate(zip(X, labels)):\n",
    "            if label == -1:\n",
    "                continue\n",
    "                \n",
    "            # a(i): Distance au centroïde de son propre cluster\n",
    "            a_i = np.linalg.norm(point - cluster_centers[label])\n",
    "            \n",
    "            # b(i): Distance au centroïde du cluster le plus proche\n",
    "            b_i = np.inf\n",
    "            for other_label in unique_labels:\n",
    "                if other_label != label:\n",
    "                    dist = np.linalg.norm(point - cluster_centers[other_label])\n",
    "                    b_i = min(b_i, dist)\n",
    "            \n",
    "            # Score de silhouette\n",
    "            if max(a_i, b_i) > 0:\n",
    "                silhouette_scores.append((b_i - a_i) / max(a_i, b_i))\n",
    "        \n",
    "          # Utiliser la médiane au lieu de la moyenne pour le score final\n",
    "        return np.median(silhouette_scores) if silhouette_scores else np.nan\n",
    "        # return np.mean(silhouette_scores) if silhouette_scores else np.nan\n",
    "\n",
    "# Calcul des métriques de qualité\n",
    "silhouette = simplified_silhouette_for_dbscan(data_scaled, labels)\n",
    "print(f\"Indice de silhouette simplifié: {silhouette:.4f}\")\n",
    "\n",
    "# # Calcul des distances intra-cluster\n",
    "# intra_distances = []\n",
    "# for label in set(labels):\n",
    "#     if label != -1:\n",
    "#         cluster_points = data_scaled[labels == label]\n",
    "#         if len(cluster_points) > 1:\n",
    "#             centroid = np.mean(cluster_points, axis=0)\n",
    "#             dists = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "#             intra_distances.append(np.mean(dists))\n",
    "\n",
    "# avg_intra = np.mean(intra_distances) if intra_distances else np.nan\n",
    "# print(f\"Distance intra-cluster moyenne: {avg_intra:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option : optimisation fine\n",
    "# Tester plusieurs valeurs d'eps autour de celle identifiée\n",
    "# eps_values = np.linspace(optimal_eps * 0.7, optimal_eps * 1.3, 7)  # ±30% autour de votre eps initial\n",
    "eps_values = np.linspace(0.033 * 0.7, 0.033 * 1.3, 7)  # ±30% autour de votre eps initial\n",
    "results = []\n",
    "\n",
    "for eps in eps_values:\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=k).fit(data_scaled)\n",
    "    labels = dbscan.labels_\n",
    "    \n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "    silhouette = simplified_silhouette_for_dbscan(data_scaled, labels)\n",
    "    \n",
    "    # Calcul des distances intra-cluster\n",
    "    intra_distances = []\n",
    "    for label in set(labels):\n",
    "        if label != -1:\n",
    "            cluster_points = data_scaled[labels == label]\n",
    "            if len(cluster_points) > 1:\n",
    "                centroid = np.mean(cluster_points, axis=0)\n",
    "                dists = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "                intra_distances.append(np.median(dists))\n",
    "    \n",
    "    avg_intra = np.median(intra_distances) if intra_distances else np.nan\n",
    "    \n",
    "    results.append({\n",
    "        'eps': eps,\n",
    "        'n_clusters': n_clusters,\n",
    "        'n_noise': n_noise,\n",
    "        'silhouette': silhouette,\n",
    "        'median_intra': avg_intra\n",
    "    })\n",
    "\n",
    "# Afficher les résultats sous forme de tableau\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Visualiser les métriques de qualité\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "axs[0].plot(results_df['eps'], results_df['silhouette'], 'o-')\n",
    "axs[0].set_title('Indice de silhouette')\n",
    "axs[0].set_xlabel('eps')\n",
    "axs[0].grid(True)\n",
    "\n",
    "axs[1].plot(results_df['eps'], results_df['n_clusters'], 'o-', label='Clusters')\n",
    "axs[1].plot(results_df['eps'], results_df['n_noise'], 'o--', label='Points de bruit')\n",
    "axs[1].set_title('Nombre de clusters et points de bruit')\n",
    "axs[1].set_xlabel('eps')\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identifier la meilleure valeur d'eps selon la silhouette\n",
    "best_idx = results_df['silhouette'].idxmax()\n",
    "best_eps = results_df.loc[best_idx, 'eps']\n",
    "print(f\"Meilleure valeur d'eps selon la silhouette: {best_eps:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparaison médiane vs moyenne (optionnel).\n",
    "#  Comparer l'approche par moyenne vs médiane\n",
    "results_comparative = []\n",
    "\n",
    "for eps in [eps_choisi]:  # Ou testez plusieurs valeurs\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=k).fit(X_scaled)\n",
    "    labels = dbscan.labels_\n",
    "    \n",
    "    # Calcul avec moyenne\n",
    "    intra_mean = []\n",
    "    for label in set(labels) - {-1}:\n",
    "        cluster_points = X_scaled[labels == label]\n",
    "        if len(cluster_points) > 1:\n",
    "            centroid = np.mean(cluster_points, axis=0)\n",
    "            dists = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "            intra_mean.append(np.mean(dists))\n",
    "    avg_intra_mean = np.mean(intra_mean) if intra_mean else np.nan\n",
    "    \n",
    "    # Calcul avec médiane\n",
    "    intra_median = []\n",
    "    for label in set(labels) - {-1}:\n",
    "        cluster_points = X_scaled[labels == label]\n",
    "        if len(cluster_points) > 1:\n",
    "            centroid = np.mean(cluster_points, axis=0)\n",
    "            dists = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "            intra_median.append(np.median(dists))\n",
    "    avg_intra_median = np.median(intra_median) if intra_median else np.nan\n",
    "    \n",
    "    # Identifier les clusters potentiellement problématiques\n",
    "    problematic_clusters = []\n",
    "    for label in set(labels) - {-1}:\n",
    "        cluster_points = X_scaled[labels == label]\n",
    "        if len(cluster_points) > 1:\n",
    "            centroid = np.mean(cluster_points, axis=0)\n",
    "            dists = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "            mean_dist = np.mean(dists)\n",
    "            median_dist = np.median(dists)\n",
    "            # Si grande différence entre moyenne et médiane\n",
    "            if mean_dist > median_dist * 1.5:\n",
    "                problematic_clusters.append(label)\n",
    "    \n",
    "    results_comparative.append({\n",
    "        'eps': eps,\n",
    "        'mean_metric': avg_intra_mean,\n",
    "        'median_metric': avg_intra_median,\n",
    "        'problematic_clusters': problematic_clusters,\n",
    "        'n_problematic': len(problematic_clusters)\n",
    "    })\n",
    "\n",
    "print(\"Comparaison moyenne vs médiane:\")\n",
    "print(pd.DataFrame(results_comparative))\n",
    "if results_comparative[0]['n_problematic'] > 0:\n",
    "    print(f\"Clusters à examiner (potentiellement avec outliers): {results_comparative[0]['problematic_clusters']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Avantages de l'utilisation de la médiane pour GC×GC-MS\n",
    "\n",
    "Résistance aux valeurs aberrantes : Les données GC×GC-MS contiennent souvent des pics aberrants ou mal définis qui peuvent fausser les moyennes.\n",
    "Meilleure représentation des pics typiques : La médiane reflète mieux la taille et forme \"typique\" des pics chromatographiques.\n",
    "Stabilité accrue : Les résultats seront moins sensibles aux variations mineures dans la préparation des échantillons ou aux artefacts d'acquisition.\n",
    "Robustesse face au bruit : Dans les régions de haute densité du chromatogramme, certains points peuvent avoir des distances anormalement élevées sans être des outliers légitimes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation du choix d eps: calcul des distances intra_ cluster (evalu la compaciteé des clusters) : on veut des cluster compacts\n",
    "from sklearn.metrics import pairwise_distances\n",
    "labels = clustering.labels_\n",
    "unique_labels = set(labels)\n",
    "unique_labels.discard(-1)  # Ignore noise\n",
    "intra_distances =[]\n",
    "for label in unique_labels:\n",
    "    cluster_points = coordinates_all_mass[labels == label]\n",
    "    if len(cluster_points) > 1:\n",
    "            centroid = np.mean(cluster_points, axis=0)\n",
    "            dists = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "            intra_distances.append(np.mean(dists))\n",
    "            overall_mean = np.mean(intra_distances) if intra_distances else np.nan\n",
    "\n",
    "            #: Plus la valeur est petite, plus les clusters sont compacts, ce qui est souvent souhaitable dans l'analyse GC×GC-MS car cela signifie que les points d'un même composé sont bien regroupés.\n",
    "            # Dans l'analyse GC×GC-MS, des clusters compacts suggèrent souvent que le regroupement correspond bien à des composés chimiques réels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distances inter cluste r: evaluer si les clusters sont bien séparés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=k)\n",
    "labels = clusterer.fit_predict(data_scaled)\n",
    "\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise = list(labels).count(-1)\n",
    "print(f\"Nombre de clusters: {n_clusters}\")\n",
    "print(f\"Nombre de points considérés comme bruit: {n_noise} ({n_noise/len(labels)*100:.1f}%)\")\n",
    "\n",
    "# Visualisation des clusters sur le plan RT1×RT2\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Pour les points de bruit, recreer l array avec des booleens\n",
    "noise_mask = labels == -1\n",
    "plt.scatter(data_scaled[noise_mask, 0], data_scaled[noise_mask, 1], color='gray', alpha=0.3, s=10, label='Bruit')\n",
    "# Pour les clusters\n",
    "unique_labels = set(labels)\n",
    "unique_labels.discard(-1)\n",
    "for label in unique_labels:\n",
    "    cluster_mask = labels == label\n",
    "    plt.scatter(data_scaled[cluster_mask, 0], data_scaled[cluster_mask, 1], alpha=0.7, s=30, label=f'Cluster {label}')\n",
    "\n",
    "plt.xlabel('RT1')\n",
    "plt.ylabel('RT2')\n",
    "plt.title(f'Résultats HDBSCAN, minPts={k}')\n",
    "# plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
